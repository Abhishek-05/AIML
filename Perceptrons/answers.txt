2)

i) As the number of points seen increases both the training and test accuracy increases. But when we cross a treshold the accuracies start to fluctuate as 
the data is not perfectly seperable using perceptrons so the perceptrons kind of osscilate.

ii) As the number of traning points is low, they can be perfectly seperated using perceptrons. But the data wont be sufficient to make predictions correctly
Therefore when the number of data points increases the training accuracy decreases and the test accuracy increases.

    1 ) Since there is no training data , all the weights will be set to zero. Therefore the label of every point will be predicted randomly. Therefore the
   accuracy will be very low. With out training the accuracy of the given dataset is about 20 percent.

3.1)

abhishek@abhishek-HP-Pavilion-Notebook:~/Desktop/classification$ python dataClassifier.py -c 1vr -t 800 -s 8000
Doing classification
--------------------
classifier:		1vr
using enhanced features?:	False
training set size:	800
Extracting features...
Training...
Starting iteration  0 ...
Starting iteration  1 ...
Starting iteration  2 ...
Testing...
5706 correct out of 8000 (71.3%).

abhishek@abhishek-HP-Pavilion-Notebook:~/Desktop/classification$ python dataClassifier.py -c 1v1 -t 800 -s 8000
Doing classification
--------------------
classifier:		1v1
using enhanced features?:	False
training set size:	800
Extracting features...
Training...
Starting iteration  0 ...
Starting iteration  1 ...
Starting iteration  2 ...
Testing...
5764 correct out of 8000 (72.0%).

They have the same accuracy when they are trained on 800 data points. The reason behind is the 1v1 classifier is not trained to its full potential. 
That is since we need to compute more information in 1v1 when compared to 1vr we need more data points for it to train well. When the data points are 
few , they will be mostly separable linearly with k hyperplanes. But as the points increase, due to the variance in the data , 1vr dosent perform as good 
as 1v1 because the points will not be linearly separable i.e one set wont be linearly separable from the rest.

abhishek@abhishek-HP-Pavilion-Notebook:~/Desktop/classification$ python dataClassifier.py -c 1vr -t 80000 -s 20000
Doing classification
--------------------
classifier:		1vr
using enhanced features?:	False
training set size:	80000
Extracting features...
Training...
Starting iteration  0 ...
Starting iteration  1 ...
Starting iteration  2 ...
Testing...
14752 correct out of 20000 (73.8%).

abhishek@abhishek-HP-Pavilion-Notebook:~/Desktop/classification$ python dataClassifier.py -c 1v1 -t 80000 -s 20000
Doing classification
--------------------
classifier:		1v1
using enhanced features?:	False
training set size:	80000
Extracting features...
Training...
Starting iteration  0 ...
Starting iteration  1 ...
Starting iteration  2 ...
Testing...
15475 correct out of 20000 (77.4%).
